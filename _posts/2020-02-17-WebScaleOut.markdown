---
layout: post
title:  "웹 서버 스케일아웃과 DB 샤딩"
date:   2020-02-12 18:00:00 +0900
categories: Rookie7
---

## 웹 서버를 두 대로 늘리면?

## 서버 한 대에서 시작

### 웹과 디비를 한 대에서 운영

* 쉽게 시작할 수 있찌만,
* 원만한 운영 어려움
* 가용성 (서버 한 대가 죽으면 백업이 없음)
* 보안성 (웹 서버는 사용자가 접근 할 수 있는 서버이기 때문에 DB는 웹 서버와 분리함)

## 두 대의 서버

### 웹 서버 1대, 디비 서버 1대

* SPOF: 웹, 디비 뭐든 하나만 죽으면 안됨

## 세 대의 서버

### 웹 서버를 2대로 늘려서

* 로브 밸런싱
    * 두 서버에 트래픽을 '분배'
* 고가용성
    * 디비 구성의 예
        * 장애 시에 대기 서버를 활용
        * 트래픽을 분배하지는 않음
        * 액티브를 평소에 사용하다가, 액티브가 죽으면 스탠바이를 사용
* 로드밸런싱과 고가용성
    * 가장 흔한 것이 L4
        * L4는 포트까지 체크 : 웹 서버가 살아있는지를 TCP로 확인함
            * 웹 서버가 요청을 받지 않아야 에러를 전송함
        * L7 헬스체크 : 헬스체크 URL로 연결해서 어떤 응답이 오는지 확인
            * 특정 URL을 호출하는 것이기 떄문에 일부러 Fail을 낼 수 있음.
            * connection을 막고 배포를 할 수 있음
    * HAProxy
        * 소프트웨어 L4
    * DNS RR을 이용
        * TTL을 아무리 짧게 해도 브라우저 캐쉬가 문제
        * 서버 쪽 JVM 캐쉬
            * JVM을 재부팅해야 되는 경우가 있음
    * 세션 공유
        * 로그인 정보 등을 공유해야함
        * 쿠키(암호화)를 이용할 수도 있지만 데이터 양에 제한
    * 웹 서버 로컬 디스크에 공유 정보를 저장할 수 없음
        * 구글 웹 엔진과 같은 자동 스케일링을 해주는 인프라의 중요 제한 사항
    * 두 서버 간 컨텐츠를 공유하려면
        * DB
        * NAS/NFS
        * Redis
* 웹 서버 두 대로 정상 서비스가 될까?
    * 각 서버가 가용 용령의 50% 이상을 받으면
        * 한 대에 문제가 생기면 다른 한대가 전체 요청을 처리할 수 없어 결국 전면 장애로 이어짐

## L4

* DSR(Direct Server Return) 모드
    * L4가 양방향 proxy라면
        * 모든 웹 서버가 받는 트래픽을 L4가 다 받아야 함
    * 서버에서 Response를 보낼 때 L4를 통하지 않고 Client로 바로 감
    * 적합한 예 : 요청 패킷이 적은 케이스
        * 일반적인 웹 요청
        * 파일 다운로드
    * 적합하지 않은 예 : 요청 패킷이 많은 케이스
        * 파일 업로드와 같은 웹 요청
        * 업로드 할 때는 L4를 거치지 않도록 예외처리
        * SMTP
    * 대용량 파일 업로드
        * 두 단계로 나눠 동작
            1. L4로 GET 요청
            2. 웹서버가 IP 반환
            3. 웹 서버로 POST 요청(파일 업로드)
        * 클라이언트에서도 제공해야 가능한 프로토콜

## 디비를 확장할 수 있으려면

* 디비의 고가용성을 고려하는 것이 아님
* 한 대로 운영하던 디비의 자원이 한계에 봉착했을 때

## 네 대의 서버

* 웹 서버 2개, 디비 마스터, 디비 슬레이브
* 단순히 디비를 늘릴 경우, 읽기는 반으로 줄지만, 쓰기가 2배로 늘어남
* 읽기 부하는 분산이 되나 복제 시간은 점점 늘어남
    * 복제본이 늘어나 안정적이지만 낭비가 큼
    * 복제 지연은 생각보다 심각

## 클러스터링과 샤딩

* 클러스터링
    * 데이터를 자동으로 분산 저장
    * 노드간 재균형 작업
* 샤딩
    * 데이터를 수동으로 분산 저장
    * 분할된 데이터는 서로 연관관계가 없음
    * 샤드 하나가 독립적으로 일을 할 수 있어야 함

## 파티셔닝/샤딩

* 데이터를 특정 속성 중심으로 물리적 분할
    * 분할된 데이터는 서로 조인을 하거나 참조가 발생해서는 안됨
    * 보통 userId, blogId, boardId 등을 사용
* 어떤 디비를 봐야할지 판단해야함
    * 매핑 디비 서버
        * 데이터 이동이 유연하지만 별도 서버 운영 필요
    * 해쉬 함수 이용
        * 데이터 분할 시 많은 데이터를 이동해야할 수 있음
        * 별도 서버 없이 찾을 수 있음
* nBaseT/CUBRID
* Vitess
    * YouTube(\~2011)
        * https://vitess.io/
        * History 부분은 전형적인 디비 스케일 아웃 역사

## 셀 아키텍처

* 데이터를 특정 속성 중심으로 물리적 분할
    * 웹, 디비 서버 이중화
    * 사용자별 어느 디비에 있는지 정보 보관
        * 어떤 웹 서버에 접근해야 하는지 확인
* 웹 서버와 디비를 묶어서 하나의 셀이라고 함
* 셀 디비
    * 분할된 데이터가 어디에 속하는지 참조
        * 전체 사용자가 공통으로 참조
        * 셀 디비, 로케이션 디비, 유저 디스커버리 서비스 등등의 용어 사용
* 장점
    * 셀 단위 스케일링
    * 장애를 특정 셀로 고립 가능
    * 프론트+백엔드 점진적 배포
        * 일부 웹 서버만 선적용하는 것은 흔하고 쉽지만
        * 디비가 변경되었을 때, 일부 웹 서버만 적용하는 것은 쉽지 않지만 셀 아키텍처에서는 가능
* 단점
    * SPOF: 셀
        * 가장 심각한 문제
        * 하지만 거의 정적인 데이터
    * 많은 장비 필요
    * 제공하는 기능에 따라 셀간 데이터를 조합해야할 수도
        * 예, 페이스북의 현 계정 친구 정보를 스트림에 추가
* 워드프레스
    * 2^n개의 버켓(샤드)을 마련
        * 가상의 버켓을 미리 많이 만들어 두고, 물리 버켓을 매핑
    * 하나의 서버에서 운영하다가 용량이 차면 2대로 분할
        * 또 차면 또 분할
* 메일
    * 웹 인터페이스
        * HTTP 리다이렉트를 이용하여 속한 셀로 전환 가능
    * IMAP, POP3
        * 프로토콜 상 어느 셀에서나 모든 사용자 서비스 가능해야함
* 몇 개의 셀이 적당?
    * 최소 2개
        * 50:50? 10:90?
            * 50:50 등분하면 점진적 배포를 적극 활용하기 어려움
            * 위험이 있는 배포를 조금 선배포하지 못하고 절반에 적용하게 됨
            * 10:90과 같이 특정 셀을 작게 가져가면 점진적 배포에 유리
    * 5개? 10개?
        * 정책!
* IDC 분할
    * 그렇게까지?
    * 4개라면 IDC별 2개씩? 혹은 1개, 3개?
        * 역시 정책!

## 디비 샤딩

1. 기존과 동일하게 서비스에 접속
2. 해당 데이터가 어느 디비에 속해 있는지 질의
3. 속한 디비에 질의

## 셀 아키텍처

1. 비로그인 상태에서 접속
2. 로그인 후 접속을 받음(Intro service)
3. 어떤 셀에 속한지 물어봄
4. 어디로 redirect할지 돌려줌
5. 해당 셀로 접속


## 참고
* NHN Basecamp